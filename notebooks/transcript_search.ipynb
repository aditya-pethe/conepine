{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from dotenv import load_dotenv\n",
    "import urllib.parse\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Video Transcript Search\n",
    "\n",
    "This notebooks goal is to find the most relevant timestamp in a video given a user's natural language description. With online video lectures, tutorials, skimming through can be tedious.\n",
    "\n",
    "This takes the following steps:\n",
    "\n",
    "1. Parsing input from youtube api - i.e, given a playlist url or video url, get the transcript\n",
    "2. Chunking the transcript + associating chunks with metadata like timestamps\n",
    "3. Doing a similarity search across those chunks given a user query\n",
    "\n",
    "Ultimately, we want to do this on a playlist level, but a single long video for now will suffice. I included some boilerplate for grabbing playlist info to extend this later.\n",
    "\n",
    "### Parse the playlist id from the url + read the playlist contents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Official tutorial playlist on pinecone\n",
    "pinecone_playlist_url = \"https://www.youtube.com/playlist?list=PLRLVhGQeJDTLiw-ZJpgUtZW-bseS2gq9-\"\n",
    "\n",
    "# Deep Dive long form tutorial on langchain agents\n",
    "langchain_video_url = \"https://www.youtube.com/watch?v=jSP-gSEyVeI\"\n",
    "\n",
    "def get_playlist_from_url(playlist_url):\n",
    "\n",
    "    parsed_url = urllib.parse.urlparse(playlist_url)\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    playlist_id = query_params[\"list\"][0]\n",
    "\n",
    "    # Get the playlist ID\n",
    "    # playlist_id = \"PLRLVhGQeJDTLiw-ZJpgUtZW-bseS2gq9-\"\n",
    "\n",
    "    url = \"https://www.googleapis.com/youtube/v3/playlistItems\"\n",
    "\n",
    "    params = {\n",
    "        'part': 'snippet',\n",
    "        'maxResults': 25,\n",
    "        'playlistId': playlist_id,\n",
    "        'key': YOUTUBE_API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def get_video_from_url(video_url):\n",
    "\n",
    "    parsed_url = urllib.parse.urlparse(video_url)\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    video_id = query_params[\"v\"][0]\n",
    "\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "\n",
    "playlist_data = get_playlist_from_url(pinecone_playlist_url)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parsing a playlist\n",
    "\n",
    "To search transcripts across a playlist, we would read all the playlist transcripts and create embeddings\n",
    "\n",
    "For now, we will focus on our longer video example, to ensure that intra-video search is solid"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "video_transcripts = {}\n",
    "\n",
    "for item in playlist_data['items']:\n",
    "\n",
    "    video_id = item['snippet']['resourceId']['videoId']\n",
    "    title = item['snippet']['title']\n",
    "    print(f\"Video ID: {video_id}, Title: {title}\")\n",
    "\n",
    "    # read transcipt of a given video\n",
    "        \n",
    "    # transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    # video_transcripts[video_id] = transcript"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Video ID: Q6616MuRmKU, Title: Pinecone #1 - Getting Started\n",
      "Video ID: DCQrrnFbLt8, Title: Pinecone #2 - Managing Indexes\n",
      "Video ID: HjeW6ed2dmI, Title: Pinecone #3 - Inserting Data\n",
      "Video ID: cqzWyNWU8oo, Title: Pinecone #4 - Managing Data\n",
      "Video ID: iWzjI0ubQEU, Title: Pinecone #5 - Querying Data\n",
      "Video ID: tn_Y19oB5bs, Title: Pinecone #6 - Metadata Filters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example Video Transcript: formatted vs raw & timestamped"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "transcript = get_video_from_url(langchain_video_url)\n",
    "\n",
    "formatter = TextFormatter()\n",
    "formatted_transcript = formatter.format_transcript(transcript).replace(\"\\n\", \" \")\n",
    "\n",
    "f\"length of formatted transcript string: {len(formatted_transcript)}\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'length of formatted transcript string: 26782'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "string_idx = 0\n",
    "\n",
    "for i,obj in enumerate(transcript):\n",
    "\n",
    "    obj[\"string_index\"] = string_idx\n",
    "    string_idx += len(obj[\"text\"]) + 1 # this + 1 is to account for the whitespace during the join\n",
    "\n",
    "    transcript[i] = obj\n",
    "\n",
    "print(\"raw transcript obj\")\n",
    "transcript[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raw transcript obj\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': 'large language models are incredibly',\n",
       " 'start': 0.0,\n",
       " 'duration': 6.12,\n",
       " 'string_index': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Search Strategy:\n",
    "\n",
    "\n",
    "Given a user query, i.e \"i want to learn how to setup a pinecone index\", return the most relevant chunk of text + a timestamp.\n",
    "\n",
    "## Chunking\n",
    "\n",
    "\n",
    "- Chunk the transcript text ~100 words?, associate metadata with each chunk like {chunk, beginning timestamp, video id}\n",
    "- Create vector embeddings from the chunk, and store the embedding + associated metadata in pinecone\n",
    "- On a user query, retrieve the most relevant chunk and meta data - call a tool to 1) Give a short answer to the user query and 2) play the video @ timestamp\n",
    "\n",
    "However, this is restricted to timstamps at each chunk. If we want the timestamp search to be more exact, we'll have to chunk smaller, or get creative. \n",
    "\n",
    "\n",
    "## Fine grain search?\n",
    "\n",
    "Approach 1: Search within chunk\n",
    "\n",
    "- Once the most relevant chunk is retrieved, do another search, possibly calling the completions api - i.e, where in this chunk is this query answered\n",
    "- Use an exact text output to retrieve the timestamp within the chunk\n",
    "\n",
    "Approach 2:\n",
    "\n",
    "- Build chunk with timestamp. I.e, every 30 seconds of video will correspond to a chunk"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# chunk by an arbitrary chunk size - a potential improvement is using spacy or NLTK as the splitter\n",
    "def chunk_by_text(text, chunk_size = 500, chunk_overlap = 20):\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap  = chunk_overlap\n",
    "    )\n",
    "\n",
    "    docs = []  # List holding all the documents\n",
    "\n",
    "    for i,chunk in enumerate(text_splitter.split_text(text)):\n",
    "        # Generate documents\n",
    "        string_index = i * (chunk_size - chunk_overlap)\n",
    "\n",
    "        docs.append(Document(\n",
    "            page_content=chunk, \n",
    "            metadata={\n",
    "                # \"string_index\": i * (chunk_size - chunk_overlap)\n",
    "                \"chunk_timestamp\": match_timestamp(string_index, transcript)\n",
    "            }))\n",
    "\n",
    "    return docs\n",
    "\n",
    "# this function basically walks through the timestamps, and finds the closest one before the given chunk\n",
    "# definitely room for optimization here - I think its o(n^2) when there is o(n) solution\n",
    "\n",
    "def match_timestamp(string_index, raw_transcript):\n",
    "\n",
    "    for i,timestamp_obj in enumerate(raw_transcript):\n",
    "\n",
    "        if string_index == 0:\n",
    "            return raw_transcript[0][\"start\"]\n",
    "            \n",
    "\n",
    "        elif timestamp_obj['string_index'] == string_index:\n",
    "            return raw_transcript[i-1][\"start\"]\n",
    "             \n",
    "\n",
    "        elif timestamp_obj['string_index'] > string_index:\n",
    "            return raw_transcript[i-1][\"start\"]\n",
    "            \n",
    "\n",
    "        elif i == len(raw_transcript) - 1:\n",
    "            return raw_transcript[-1][\"start\"]\n",
    "            \n",
    "        \n",
    "\n",
    "chunked_text = chunk_by_text(formatted_transcript)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(page_content=\"large language models are incredibly powerful as we've seen but they lack some of the abilities that even the dumbest computer programs can handle with ease logic calculations and search are just a few examples of where large language models fail and really dumb computer programs um can actually perform very well we've been using computers to solve incredibly complex calculations for a very long time yet if we ask gbt4 to tell us the answer to what is 4.1 multiplied by 7.9 it actually fails\", metadata={'chunk_timestamp': 0.0}),\n",
       " Document(page_content=\"it actually fails isn't it fascinating that simple calculator program can do this but what is probably the most sophisticated AI program in the world right now that is accessible by us cannot and that's not all if I ask GT4 my small overused example by now of how do I use the LM in line chain it struggles again it's true that line chain was a blockchain project yeah there didn't seem to be any nlm chain component nor Lang tokens these are both hallucinations granted the reason that gbt4 is\", metadata={'chunk_timestamp': 37.68}),\n",
       " Document(page_content=\"reason that gbt4 is unable to tell us about these is because it hasn't heard of Lang chain or at least not line chain I'm referring to that is because Jeep T4 has no connection to the outside world the only part of the outside world that gpt4 has seen is what it saw during its training and the training day to cut off of GT4 appears to be around September 2021 with what seemed to be major weaknesses in today's large language models we need to find Solutions One Suite of potential solutions to\", metadata={'chunk_timestamp': 73.32}),\n",
       " Document(page_content=\"solutions to these problems comes in the form of Agents these agents don't just solve many of the problems we saw above but actually many others as well in fact by using agents we actually have a almost unlimited upside in the potential of what we can do with large language models so we're going to learn what agents are and how we can use them within line chain library to superpower our large language models what we'll do is I'll quickly go through an introduction to agents in line chain and\", metadata={'chunk_timestamp': 111.299}),\n",
       " Document(page_content=\"in line chain and then I'll hand it over to friends history for more of a deep dive into agents in line train so let's jump straight into it we can think of Agents as enabling tools for large language models kind of like how a human like I would use a calculator for maps or I might perform a Google search for information agents allow a large language model to do the same thing using agents a large language model can write and execute python code it can perform Google search and it can even\", metadata={'chunk_timestamp': 146.58}),\n",
       " Document(page_content=\"and it can even perform SQL queries let's start with a very simple example of this what we're going to do is build a calculator agent that can also handle some general knowledge queries now to use agents in line chain we need three key components that is a large language model or multiple large language models a tool that we will be interacting with and an Asian to control the interaction let's start by installing Line train and initializing our large language model so we're in collab here\", metadata={'chunk_timestamp': 179.879}),\n",
       " Document(page_content=\"in collab here there will be a link to this notebook somewhere in the top of the video right now and we do a PIV install Lang chain and open AI because we're going to be using open ai's large language models here but you can replace this you can use cohere you can use I can face models and I'm not sure if it's implemented yet but I'm sure pretty soon there'll probably be Google Palm models in here as well Okay cool so we need to first start by initializing Our Lives orange model we're using\", metadata={'chunk_timestamp': 220.019}),\n",
       " Document(page_content=\"model we're using text avengers003 here you can replace that obviously with more recent models as well okay I've already run this with my API key in there so move on next we want to initialize what is going to be a calculator tool using the LM math chain so the element math chain if you watch the previous videos in this series you you've probably you've probably seen it it is basically a large language model that will call to python with some code for performing a calculation okay that's what\", metadata={'chunk_timestamp': 253.019}),\n",
       " Document(page_content=\"okay that's what this is here right and what we're doing here is we're formatting it into what we call a tool now a tool it is simply the the run so the functionality of this llm math chain plus a name for that chain for that function and also description now this description is useful this is essentially The Prompt this will be included in the prompt for the agent executor a large launch model and we're basically going to say look this is a tool that you can use right it's called calculator\", metadata={'chunk_timestamp': 288.479}),\n",
       " Document(page_content=\"called calculator and you should use it when you think this is relevant okay so we're saying it's useful for when you need to answer questions about math right that's how the large language model will know when to use this tool now when we're passing those tools to the large language model we will actually pass a list of tools okay so for now there's just one item in that list but we can add multiple as we'll see very soon now I'm using this tool is we need to initialize an agent that will\", metadata={'chunk_timestamp': 328.5}),\n",
       " Document(page_content=\"an agent that will contain them it's going to contain like a base knowledge model which is going to control it's kind of like the orchestrator at the top level large language model and there are also these different types of Agents okay so this one is a zero shot react description agent that means a few things so zero shot part of that means that the agent is currently looking at just the current prompt it doesn't have any memory all right so if you've got a chat bot you probably wouldn't\", metadata={'chunk_timestamp': 360.62}),\n",
       " Document(page_content=\"probably wouldn't necessarily use this one but we'll cover some Alternatives later on so for it being zero shot it doesn't have that memory and it's focusing on the current action only react is a like an agent framework the idea behind that is that we or the agent will reason about whatever prompt has been assigned to it it will decide on an action to take and then it will pass some information to that action item that action item is the tools that we're using here and then it will get a\", metadata={'chunk_timestamp': 396.12}),\n",
       " Document(page_content=\"then it will get a response from the action item that tool and then repeat this process of reasoning and action we're not going to go too into detail on that here that's kind of deserves its own video I think and as for which action to take the Asian is basing that on the description of each of the tools that we have okay so let's initialize that one other important thing here is these Max iterations so I kind of mentioned just now it can go through a loop of reason and action we add match\", metadata={'chunk_timestamp': 433.28}),\n",
       " Document(page_content=\"action we add match iteration so that it doesn't just go into an infinite Loop of reasoning and making actions and so on and so on just forever right so we kind of limit it to you can do three of these loops and then cuts off Okay cool so let's ask some questions so this agent we can see like the thought process of the agent here right so it's saying okay I need to calculate this expression so what is the action I only take I need to use a calculator and what's the input I'm going to pass to\", metadata={'chunk_timestamp': 468.419}),\n",
       " Document(page_content=\"going to pass to this tool it is this here right this is our what we've assets calculate here okay and then the response from that agent from the L M map chain is this here okay this number okay and it says the next 40 it has is I now know the final answer it is this right and then we finish the chain oh like that Asian execution chain so the import was this and the output we get is this right now the end user doesn't need to see all of this the reason that we're seeing all of this is because\", metadata={'chunk_timestamp': 501.96}),\n",
       " Document(page_content=\"of this is because we set verbose equal to True set list of false we'll just get the output right but obviously we're developing this we want to see what is actually happening now what you can see I've already run this the output the actual actual value here is in fact this number here okay so that's much better than when we're using a large language models typically and they just give us like a completely wrong answer all right so that looks good let's try something else now here we're kind of\", metadata={'chunk_timestamp': 539.88}),\n",
       " Document(page_content=\"here we're kind of using natural language to kind of Define the calculation that needs to needs to happen we say Mary has four apples JoJo brings two and a half apple boxes where an Apple box contains eight apples how many apples do we have right I mean it's just kind of simple logic and calculation here but naturally it needs to use both the calculator chain dot llm matching and also its own sort of LM ability to reason about what is happening here so it starts by I need to figure out how many\", metadata={'chunk_timestamp': 573.26}),\n",
       " Document(page_content=\"figure out how many apples are in the boxes all right so it's doing this step by step which I think is really cool so it says there's two and a half boxes and each watch contains eight apples so there's 20 apples in the boxes and it says I need to add the apples Mary has to the apples and the boxes okay we do calculate against four plus twenty twenty four and now it's like I now have the final answer we have 24 apples cool so we see the input and we see the Apple we have 24 apples great all of\", metadata={'chunk_timestamp': 612.06}),\n",
       " Document(page_content=\"apples great all of this so far that we've done here a LM math chain could probably do it could definitely do this and there's probably a good chance it could do this it wouldn't go through the multiple reasoning steps but it might be able to do this in a single step so with that in mind why don't we just use the llm math chain well what if we ask this what is the capital of Norway an LM math Chain by itself is not going to be able to answer this because it's going to go to the calculator right\", metadata={'chunk_timestamp': 642.959}),\n",
       " Document(page_content=\"calculator right so we say I need to look at the answer okay so right now it's what is it doing this is something new that's because it's seeing what is the capital of Norway in the the base or the agent executed prompt is saying to answer this prompt you need to use one of the following tools but the only tool that we're given it is a calculator the LM knows that a calculator isn't going to give it the answer to what is the capital of Norway so it's actually hallucinating and trying to find\", metadata={'chunk_timestamp': 683.1}),\n",
       " Document(page_content=\"and trying to find another tool that it believes would help and look up tool would help here but it doesn't actually have access to a lookup tool it's just kind of imagining that it does it's hallucinating that it does okay so it's like okay the only tool I actually have here is a calculator and obviously you pass this to the calculator and the the answer is it's not going to work right so it actually just says action input n a I'm like I can't I don't even know what to give to this calculator\", metadata={'chunk_timestamp': 711.72}),\n",
       " Document(page_content=\"to this calculator and then we'll get a value error okay right that's fine it's kind of expensive we only have a calculator for this agent to use it's not going to be able to answer this question but what if we do want it to be able to answer General Knowledge Questions as well as perform calculations well okay in that case what we need to do is add another tool to the agents toolbox what we're going to do is just initialize a simple LM chain right so to answer what is the capital of Norway a\", metadata={'chunk_timestamp': 743.339}),\n",
       " Document(page_content=\"capital of Norway a simple llm can do that right so all we're going to do is create this LM chain uh we're not doing any we're not already doing anything here we just got a problem template which is just going to pass a query straight to the llm chain okay and we're going to call this one the language model and we're going to say use this tool for general purpose queries and logic okay cool and we just add a new tool to our tools list like this right so now I've got two tools in that tool list\", metadata={'chunk_timestamp': 775.98}),\n",
       " Document(page_content='in that tool list and we just reinitialize the agent with our two tools the calculator and the language model okay now we say what is the capital of Norway and it should say ah okay I can I can refer to the language model for this and we say what is the capital Norway the capital of Norway is also okay so we get the correct answer this time and yeah I now need a final answer Capital no way then we can ask it a math question okay what is this okay so it can answer math questions as well now so', metadata={'chunk_timestamp': 809.82}),\n",
       " Document(page_content=\"as well now so we've all of a sudden our agent is able to do two completely different things that we would need separate LM chains or chains four which I think is really cool and these are two super simple examples Francisco is in a moment going to go through I think what a far more interesting examples and you'll definitely see more of how we can use these agents in line chain but before I finish there is just one thing I should point out because I kind of miss it for the sake of Simplicity\", metadata={'chunk_timestamp': 841.139}),\n",
       " Document(page_content=\"sake of Simplicity but it wouldn't be fair for me to not mention it is up here we defined our our math tool here right in reality there is already a math tool and a set of pre-built tools that come with light chain and to use those we would write something like from Langston rain agents Imports load tools and from there we just do tools load tools and then here we actually pass a list of the tools that we would like to load so if we just want the nlm math chain again redirect that now the LM\", metadata={'chunk_timestamp': 875.94}),\n",
       " Document(page_content=\"that now the LM math chain does require llm so we also need to pass a lamin there as well right so let me let's run this again if we look at tools name and tools description you see we have calculator useful when you need to answer questions about that and if you print out a full tool list it's going to show you a ton of other things as well including your open AI API key which is is very useful for when you're trying trying to show people what is in there so if we try the same again with this\", metadata={'chunk_timestamp': 917.94}),\n",
       " Document(page_content=\"again with this we can let's just copy and we'll see that we actually get the same thing with the same tool name with the same description and if you print out for the thing you'll also see all of the parameters that Define the the tool and chain being used so these two bits of code here do the exact same thing right I'm just saying that you can initialize the tool by yourself or you can use the pre-book tools as well now I think I've talked for long enough what we'll do is we'll pass it over\", metadata={'chunk_timestamp': 962.1}),\n",
       " Document(page_content=\"we'll pass it over to Francisco who's going to take us through these tools and agents in a lot more detail so over to Francisco Thanks James for that introduction and now we will be deep diving into agents agents are arguably the most important building block in Lang chain so it's really important that we get them right and we'll be seeing a few examples to really understand how they work as always we need to initialize our openai LM and we will get into the definition here so the official\", metadata={'chunk_timestamp': 992.699}),\n",
       " Document(page_content=\"so the official definition is that agents use LMS to determine which actions to take and in what order and an action can be using a tool or returning to the user but if we think about about it more intuitively what an agent does is it applies reasoning to use several tools collectively and Unison to build an answer for the user so this is the key behind the engines it's the agents are reasoning about what tools they need to use and how they need to use them and how they need to combine their\", metadata={'chunk_timestamp': 1025.339}),\n",
       " Document(page_content=\"to combine their outputs to actually give a right answer and this is a really really powerful framework as we will see in the next examples so whenever we'll be creating a database it's not really important to understand exactly what we're doing to create this database the important thing here is that we're going to build an SQL database with one table which is a table with stocks and we will add some observations because we would want our agent to interact with these observations here we have\", metadata={'chunk_timestamp': 1061.46}),\n",
       " Document(page_content='here we have a few stocks two stocks actually with the different prices in different day times and the important part where we will be creating the tool for the agent to use so here we create a chain which uses the database we just created and this is the engine we just created over here and we will create a database chain from that engine now we will build a tool and here just a small definition of what a tool is a tool is a function or a method that the agent can use when it thinks it is', metadata={'chunk_timestamp': 1091.76}),\n",
       " Document(page_content=\"it thinks it is necessary so how will the agent know if it's necessary well we're giving it a description here so we're telling the agent when it should be used here it here we're giving the agent the function it should run and here's the name so he will the agent will ask our chain for a question about stocks and prices and the our chain will answer that question using the data from the database so just a few clarifications before we dive into our first agent type we will see different agent\", metadata={'chunk_timestamp': 1135.2}),\n",
       " Document(page_content=\"see different agent types in this deep dive but each one has three variables so we have to define the tools we want to give the agent the LM because all agents are llm based and the agent type so what type of agent we want to use we will start with with this agent with this zero shot react agent and we'll first need to initialize our agent and this agent will be able to basically reason about our question uh gather information from tools and answer that's what the theory shot agent does so here\", metadata={'chunk_timestamp': 1170.24}),\n",
       " Document(page_content='agent does so here we will load one tool which is the llm math tool which we already saw with James and we will append the SQL tool we saw before and as the name suggests here we will use this agent to perform zero shot tasks so we will not have many interactions with the with this agent but only one or at least we will have different interactions but they will be isolated from each other and we will be asking questions that can be answered with the tools and the agent will help us answer them', metadata={'chunk_timestamp': 1208.64}),\n",
       " Document(page_content=\"help us answer them one note here it's important to have in mind that we should always set the max iterations so the max iterations with the react agents basically means how many thoughts it can have about our question and about using tools so basically what this agent does is it thinks about what it needs to do and then it does it and one of the actions it can do is refer to one of the tools so just to avoid the agent getting into an infinite Loop and using tools indefinitely we should always\", metadata={'chunk_timestamp': 1240.799}),\n",
       " Document(page_content=\"we should always set the max iterations that we're comfortable with and depending on the use case that might change but it's something that is useful to take into account here we will set max iterations to three all right so here we will create our agent and we will ask it a very complex question which is what is the multiplication of the ratio between stock prices for ABC and XYZ in two different dates so it's it involves quite a lot of steps and we will try to understand what the agent is\", metadata={'chunk_timestamp': 1271.82}),\n",
       " Document(page_content='what the agent is doing here so first it needs to compare the prices or it knows it needs to compare these prices on two different days it queries the SQL database for the third and the fourth of January with the prices for ABC and fyz here we can see that the query is generated and we get the results and then the agent is getting the answer from this tool which Returns the actual data that was requested which is this piece here now the agent with this information is determining that these are', metadata={'chunk_timestamp': 1301.22}),\n",
       " Document(page_content=\"that these are the the right prices and now it needs to make a calculation so how will it make this calculation with the calculator let's see what it's doing it's it's calculating the ratio first of the two prices on each day and then when it has the two ratios with the calculator it's using calculator another time to calculate the multiplication between the two ratios so this framework is really powerful because as I said before it's combining reasoning with the tools and in several steps it's\", metadata={'chunk_timestamp': 1337.64}),\n",
       " Document(page_content=\"several steps it's finding and converging to the right answer by using these tools so this is the key takeaway here for this zero shot react description agent we can see the prompt just as a quick pass here what we're teaching it to do is we're sending it obviously the tools that it needs to to use and then we're we're asking it to use this question thought action action input observation framework and this framework will can be repeated and N times until it knows the final answer or it reaches\", metadata={'chunk_timestamp': 1367.28}),\n",
       " Document(page_content='or it reaches Max iterations which we referred to earlier this is a very powerful framework you can check it out in the miracle paper which we link below but the important thing here to know is that it enables us to combine reasoning with tools and this is the type of things that you can do with agents the level of abstraction is much higher than using a tool in isolation so basically the llm now has the ability to reason how to best use this tool and basically another thing that is important', metadata={'chunk_timestamp': 1400.039}),\n",
       " Document(page_content=\"that is important here is the age and scratch Pad within the prompt and that is where every thought and action that the agent has already performed will be appended and and thus the agent will know at each point in time what it has found out until that moment and will be able to continue that thought process from there so that's where the agent notes uh its previous thoughts during the thought process okay so now we're ready for our second type of agent this type of agent is really similar to\", metadata={'chunk_timestamp': 1433.76}),\n",
       " Document(page_content=\"really similar to the first one so let's take a look this is called conversational react and this is basically the last agent but with memory so we can interact with it in several interactions and ask it questions from things that we have already said it is really useful to have a chatbot basically it's the basic basis for for chatbots in Lang chain so it's it's really useful again we load the same tools here we will add memory as as we said so we will ask it a similar question a bit different\", metadata={'chunk_timestamp': 1463.1}),\n",
       " Document(page_content=\"a bit different a bit simpler and we asked the previous agent so we will ask it what the ratio of stock prices is on January the 1st for these two stocks let's see what it answers so it's getting the stock data and it calculated without the calculator since it's a simple calculation the actual ratio we need so let's see the prompt here so so we will see the prompt is quite similar But it includes this uh prefix we could say where we are telling the agent that it is an assist assistant then it\", metadata={'chunk_timestamp': 1497.539}),\n",
       " Document(page_content=\"assistant then it can assist with a wide range of tasks and basically that it's here to assist and to to answer questions or have conversations so this is the main thing behind this agent and also we can see here we have a chat history variable where we will be including the memory for this agent so these are the main differences and if we ask it exactly the same question as we asked the previous agent let's see what what happens it's using the chain it's getting the data but we are getting the\", metadata={'chunk_timestamp': 1528.86}),\n",
       " Document(page_content=\"we are getting the right answer but the action input for the chain was all ready to get the ratio and then it didn't use the tool to multiply it just multiplied with on its own right without using a tool so it made this decision of not using the calculator whereas our previous agent had decided to use a calculator here and these two agents sometimes don't behave exactly the same because the problems are are different perhaps for this agent we're telling it that it can solve a wide range of\", metadata={'chunk_timestamp': 1568.7}),\n",
       " Document(page_content=\"a wide range of tasks so maybe it's getting the confidence to try out some math there but in essence what they're doing is quite similar but this agent is including a conversational aspect and a memory aspect all right now we will see our final two agents and this first one is called the react doc store agent and it is made to interact with a document store so let's say that we want to interact with Wikipedia and we will need to to send it to tools one for searching articles and the other one\", metadata={'chunk_timestamp': 1600.26}),\n",
       " Document(page_content=\"and the other one is for looking up specific terms in the article it found and this is if we think about it what we do when we search a document solar Wikipedia we search for an article that might have the answer to our question and then we we search within the article for the specific paragraph or snippet where the question is so we will do exactly that we will initialize this agent here and we will run what were committee's last words let's see so it's entering the chain it searches for\", metadata={'chunk_timestamp': 1633.919}),\n",
       " Document(page_content=\"it searches for Archimedes it's interesting and let's see so it has its first observation this observation is the first paragraph of the article it didn't find the answer there and then it looks up last words and it finds the answer within that document so that that's basically how it works we can see more about this in the prompt there's a few examples we will not print it because it's really large but you can do so and here is the paper for this agent as well so yeah this this agent is\", metadata={'chunk_timestamp': 1663.799}),\n",
       " Document(page_content=\"this this agent is basically useful for interacting with very large document source and we can think here that this is basically the same thing that we would do we would do so the agent does it for us basically and finally we have this last agent which is called the self ask with search and basically here the name is telling us a lot about how this agent works it's asking questions which are not the last the user question obviously but there are intermediate questions that it's asking to\", metadata={'chunk_timestamp': 1698.6}),\n",
       " Document(page_content=\"that it's asking to understand all the pieces of data it needs to build the last answer it needs to give to the user so why will it ask all these follow-up questions on the user's original query because it will search it will use a search engine to find the intermediate answers and then build to the last final answer to the user so as we can see here we need to send it one tool which is called the intermediate answer tool and it will search it must be some kind of search here we are using\", metadata={'chunk_timestamp': 1730.58}),\n",
       " Document(page_content='here we are using Google search we will not showcase this functionality for that you need an API Key by using the serp API wrapper but we will initialize this agent and we will see its prompt which is basically enough to understand how it works so here we can see how it works so the question original question would be who lived longer Muhammad Ali or Alan Turing and the follow-up questions needed here it determines that it needs follow-up questions and it starts asking them so how old was', metadata={'chunk_timestamp': 1761.86}),\n",
       " Document(page_content=\"them so how old was Muhammad Ali when he died intermediate answer found by searching Muhammad Ali was 74 years old how old was Alan Turing and the Turing was 41 years old so the final answer is Muhammad Ali so this is the kind of logic that the agent follows and to get these intermediate answers it needs some kind of Search tool this is one option that Lang chain provides there might be others but it needs to be able to search and that's the main characteristic of this agent which is that it\", metadata={'chunk_timestamp': 1793.58}),\n",
       " Document(page_content=\"which is that it gets intermediate answers to follow-up questions by searching so you have the paper here also if you want to dive deeper and this is it uh we're wrapping up for generic agents these are the main agents in Lang chain there are others as well and as I mentioned briefly earlier you can check out agent toolkits for example there will be others in the future too it's really worthwhile to to see the docs and follow closely the developments that there are in launching and there are\", metadata={'chunk_timestamp': 1824.419}),\n",
       " Document(page_content=\"and there are things you can do you can create your own agent you can use agents with several other tools and another thing worth mentioning is that you can use a tracing UI tool that is within Lang chain which will allow you to understand within a beautiful UI how the agent is thinking on what different calls to different llms it did within its thought process so that is really convenient when you're using complex agents with several tools and it might be tricky to track what the whole thought\", metadata={'chunk_timestamp': 1857.72}),\n",
       " Document(page_content='the whole thought process and and what answers intermediate answers did it get so that is really recommended if you are using agents with in a little bit more complex scenarios so this is it for agents here in Lang chain series and I hope you really enjoyed this topic I think again this is probably the most important topic in Bank chain and the most interesting one so yeah see you in the next one thank you', metadata={'chunk_timestamp': 1888.44})]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "chunked_text[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(page_content=\"large language models are incredibly powerful as we've seen but they lack some of the abilities that even the dumbest computer programs can handle with ease logic calculations and search are just a few examples of where large language models fail and really dumb computer programs um can actually perform very well we've been using computers to solve incredibly complex calculations for a very long time yet if we ask gbt4 to tell us the answer to what is 4.1 multiplied by 7.9 it actually fails\", metadata={'chunk_timestamp': 0.0})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create embeddings and do similarity search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(chunked_text, embeddings)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment\n",
    "\n",
    "To experiment, change the query and run the following two cells - you can go to the reference video here:\n",
    "\n",
    "https://www.youtube.com/watch?v=jSP-gSEyVeI\n",
    "\n",
    "The last cell will return a link to the predicted timestamp, and say the timestamp too. Double check, because sometimes\n",
    "youtube caches your video progress and overrides the timestamp parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "query = \"How do I create a langchain agent\"\n",
    "similar_docs = db.similarity_search_with_score(query)\n",
    "\n",
    "similar_docs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(Document(page_content=\"and there are things you can do you can create your own agent you can use agents with several other tools and another thing worth mentioning is that you can use a tracing UI tool that is within Lang chain which will allow you to understand within a beautiful UI how the agent is thinking on what different calls to different llms it did within its thought process so that is really convenient when you're using complex agents with several tools and it might be tricky to track what the whole thought\", metadata={'chunk_timestamp': 1857.72}),\n",
       "  0.3301338),\n",
       " (Document(page_content=\"and it can even perform SQL queries let's start with a very simple example of this what we're going to do is build a calculator agent that can also handle some general knowledge queries now to use agents in line chain we need three key components that is a large language model or multiple large language models a tool that we will be interacting with and an Asian to control the interaction let's start by installing Line train and initializing our large language model so we're in collab here\", metadata={'chunk_timestamp': 179.879}),\n",
       "  0.37767547),\n",
       " (Document(page_content=\"solutions to these problems comes in the form of Agents these agents don't just solve many of the problems we saw above but actually many others as well in fact by using agents we actually have a almost unlimited upside in the potential of what we can do with large language models so we're going to learn what agents are and how we can use them within line chain library to superpower our large language models what we'll do is I'll quickly go through an introduction to agents in line chain and\", metadata={'chunk_timestamp': 111.299}),\n",
       "  0.38365442),\n",
       " (Document(page_content=\"we'll pass it over to Francisco who's going to take us through these tools and agents in a lot more detail so over to Francisco Thanks James for that introduction and now we will be deep diving into agents agents are arguably the most important building block in Lang chain so it's really important that we get them right and we'll be seeing a few examples to really understand how they work as always we need to initialize our openai LM and we will get into the definition here so the official\", metadata={'chunk_timestamp': 992.699}),\n",
       "  0.39251262)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "best_doc = similar_docs[-1][0]\n",
    "seconds = best_doc.metadata[\"chunk_timestamp\"]\n",
    "s = f\"https://www.youtube.com/watch?v=jSP-gSEyVeI&t={seconds}\"\n",
    "\n",
    "def convert_seconds(seconds):\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    return int(minutes), int(seconds)\n",
    "\n",
    "# Example usage:\n",
    "minutes, seconds = convert_seconds(seconds)\n",
    "\n",
    "\n",
    "print(s)\n",
    "print(f\"exact timestamp - {minutes}:{seconds}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.youtube.com/watch?v=jSP-gSEyVeI&t=992.699\n",
      "exact timestamp - 16:32\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.11.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11.3 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}