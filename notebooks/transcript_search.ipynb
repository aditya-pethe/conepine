{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from dotenv import load_dotenv\n",
    "import urllib.parse\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Video Transcript Search\n",
    "\n",
    "This notebooks goal is to find the most relevant timestamp in a video given a user's natural language description. With online video lectures, tutorials, skimming through can be tedious.\n",
    "\n",
    "This takes the following steps:\n",
    "\n",
    "1. Parsing input from youtube api - i.e, given a playlist url or video url, get the transcript\n",
    "2. Chunking the transcript + associating chunks with metadata like timestamps\n",
    "3. Doing a similarity search across those chunks given a user query\n",
    "\n",
    "Ultimately, we want to do this on a playlist level, but a single long video for now will suffice. I included some boilerplate for grabbing playlist info to extend this later.\n",
    "\n",
    "### Parse the playlist id from the url + read the playlist contents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Official tutorial playlist on pinecone\n",
    "pinecone_playlist_url = \"https://www.youtube.com/playlist?list=PLRLVhGQeJDTLiw-ZJpgUtZW-bseS2gq9-\"\n",
    "\n",
    "# Deep Dive long form tutorial on langchain agents\n",
    "langchain_video_url = \"https://www.youtube.com/watch?v=jSP-gSEyVeI\"\n",
    "\n",
    "def get_playlist_from_url(playlist_url):\n",
    "\n",
    "    parsed_url = urllib.parse.urlparse(playlist_url)\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    playlist_id = query_params[\"list\"][0]\n",
    "\n",
    "    # Get the playlist ID\n",
    "    # playlist_id = \"PLRLVhGQeJDTLiw-ZJpgUtZW-bseS2gq9-\"\n",
    "\n",
    "    url = \"https://www.googleapis.com/youtube/v3/playlistItems\"\n",
    "\n",
    "    params = {\n",
    "        'part': 'snippet',\n",
    "        'maxResults': 25,\n",
    "        'playlistId': playlist_id,\n",
    "        'key': YOUTUBE_API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def get_video_from_url(video_url):\n",
    "\n",
    "    parsed_url = urllib.parse.urlparse(video_url)\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    video_id = query_params[\"v\"][0]\n",
    "\n",
    "    return YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "\n",
    "playlist_data = get_playlist_from_url(pinecone_playlist_url)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parsing a playlist\n",
    "\n",
    "To search transcripts across a playlist, we would read all the playlist transcripts and create embeddings\n",
    "\n",
    "For now, we will focus on our longer video example, to ensure that intra-video search is solid"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "video_transcripts = {}\n",
    "\n",
    "for item in playlist_data['items']:\n",
    "\n",
    "    video_id = item['snippet']['resourceId']['videoId']\n",
    "    title = item['snippet']['title']\n",
    "    print(f\"Video ID: {video_id}, Title: {title}\")\n",
    "\n",
    "    # read transcipt of a given video\n",
    "        \n",
    "    # transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    # video_transcripts[video_id] = transcript"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Video ID: Q6616MuRmKU, Title: Pinecone #1 - Getting Started\n",
      "Video ID: DCQrrnFbLt8, Title: Pinecone #2 - Managing Indexes\n",
      "Video ID: HjeW6ed2dmI, Title: Pinecone #3 - Inserting Data\n",
      "Video ID: cqzWyNWU8oo, Title: Pinecone #4 - Managing Data\n",
      "Video ID: iWzjI0ubQEU, Title: Pinecone #5 - Querying Data\n",
      "Video ID: tn_Y19oB5bs, Title: Pinecone #6 - Metadata Filters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example Video Transcript: formatted vs raw & timestamped"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "transcript = get_video_from_url(langchain_video_url)\n",
    "\n",
    "formatter = TextFormatter()\n",
    "formatted_transcript = formatter.format_transcript(transcript).replace(\"\\n\", \" \")\n",
    "\n",
    "f\"length of formatted transcript string: {len(formatted_transcript)}\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'length of formatted transcript string: 26782'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "string_idx = 0\n",
    "\n",
    "for i,obj in enumerate(transcript):\n",
    "\n",
    "    obj[\"string_index\"] = string_idx\n",
    "    string_idx += len(obj[\"text\"]) + 1 # this + 1 is to account for the whitespace during the join\n",
    "\n",
    "    transcript[i] = obj\n",
    "\n",
    "print(\"raw transcript obj\")\n",
    "transcript[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raw transcript obj\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': 'large language models are incredibly',\n",
       " 'start': 0.0,\n",
       " 'duration': 6.12,\n",
       " 'string_index': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Search Strategy:\n",
    "\n",
    "\n",
    "Given a user query, i.e \"i want to learn how to setup a pinecone index\", return the most relevant chunk of text + a timestamp.\n",
    "\n",
    "## Chunking\n",
    "\n",
    "\n",
    "- Chunk the transcript text ~100 words?, associate metadata with each chunk like {chunk, beginning timestamp, video id}\n",
    "- Create vector embeddings from the chunk, and store the embedding + associated metadata in pinecone\n",
    "- On a user query, retrieve the most relevant chunk and meta data - call a tool to 1) Give a short answer to the user query and 2) play the video @ timestamp\n",
    "\n",
    "However, this is restricted to timstamps at each chunk. If we want the timestamp search to be more exact, we'll have to chunk smaller, or get creative. \n",
    "\n",
    "\n",
    "## Fine grain search?\n",
    "\n",
    "Approach 1: Search within chunk\n",
    "\n",
    "- Once the most relevant chunk is retrieved, do another search, possibly calling the completions api - i.e, where in this chunk is this query answered\n",
    "- Use an exact text output to retrieve the timestamp within the chunk\n",
    "\n",
    "Approach 2:\n",
    "\n",
    "- Build chunk with timestamp. I.e, every 30 seconds of video will correspond to a chunk"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# chunk by an arbitrary chunk size - a potential improvement is using spacy or NLTK as the splitter\n",
    "def chunk_by_text(text, chunk_size = 500, chunk_overlap = 20):\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap  = chunk_overlap\n",
    "    )\n",
    "\n",
    "    docs = []  # List holding all the documents\n",
    "\n",
    "    for i,chunk in enumerate(text_splitter.split_text(text)):\n",
    "        # Generate documents\n",
    "        string_index = i * (chunk_size - chunk_overlap)\n",
    "\n",
    "\n",
    "        docs.append(Document(\n",
    "            page_content=chunk, \n",
    "            metadata={\n",
    "                # \"string_index\": i * (chunk_size - chunk_overlap)\n",
    "                \"chunk_timestamp\": match_timestamp(string_index, transcript),\n",
    "                \"video_id\":video_id\n",
    "            }))\n",
    "\n",
    "    return docs\n",
    "\n",
    "# this function basically walks through the timestamps, and finds the closest one before the given chunk\n",
    "# definitely room for optimization here - I think its o(n^2) when there is o(n) solution\n",
    "\n",
    "def match_timestamp(string_index, raw_transcript):\n",
    "\n",
    "    for i,timestamp_obj in enumerate(raw_transcript):\n",
    "\n",
    "        if string_index == 0:\n",
    "            return raw_transcript[0][\"start\"]\n",
    "            \n",
    "\n",
    "        elif timestamp_obj['string_index'] == string_index:\n",
    "            return raw_transcript[i-1][\"start\"]\n",
    "             \n",
    "\n",
    "        elif timestamp_obj['string_index'] > string_index:\n",
    "            return raw_transcript[i-1][\"start\"]\n",
    "            \n",
    "\n",
    "        elif i == len(raw_transcript) - 1:\n",
    "            return raw_transcript[-1][\"start\"]\n",
    "            \n",
    "        \n",
    "\n",
    "chunked_text = chunk_by_text(formatted_transcript)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "chunked_text[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(page_content=\"large language models are incredibly powerful as we've seen but they lack some of the abilities that even the dumbest computer programs can handle with ease logic calculations and search are just a few examples of where large language models fail and really dumb computer programs um can actually perform very well we've been using computers to solve incredibly complex calculations for a very long time yet if we ask gbt4 to tell us the answer to what is 4.1 multiplied by 7.9 it actually fails\", metadata={'chunk_timestamp': 0.0, 'video_id': 'tn_Y19oB5bs'})"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create embeddings and do similarity search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(chunked_text, embeddings)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from uuid import uuid4\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def create_embeddings(chunked_text):\n",
    "\n",
    "    ids = []\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "\n",
    "    for i,chunk in enumerate(chunked_text):\n",
    "\n",
    "        text = chunk.page_content\n",
    "        chunk_metadata = chunk.metadata\n",
    "\n",
    "        chunk_metadata[\"chunk_index\"] = i\n",
    "        chunk_metadata[\"text\"] = text\n",
    "\n",
    "        metadatas.append(chunk_metadata)\n",
    "    \n",
    "    ids = [str(uuid4()) for _ in chunked_text]\n",
    "    embeddings = embed.embed_documents([c.page_content for c in chunked_text])\n",
    "\n",
    "    return ids, embeddings, metadatas\n",
    "        \n",
    "\n",
    "ids, embeddings, metadatas = create_embeddings(chunked_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "#  index video and upsert into pinecone\n",
    "\n",
    "pinecone.init(\n",
    "api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "environment=PINECONE_ENV,  # next to api key in console\n",
    ")\n",
    "\n",
    "index = pinecone.Index(\"video-transcript-embeddings\")\n",
    "index.upsert(vectors=zip(ids,embeddings,metadatas))\n",
    "\n",
    "# docsearch = Pinecone.from_existing_index('video-transcript-embeddings', embeddings)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'describe_index'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m index\u001b[38;5;241m.\u001b[39mupsert(vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mzip\u001b[39m(ids,embeddings,metadatas))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# docsearch = Pinecone.from_existing_index('video-transcript-embeddings', embeddings)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe_index\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'describe_index'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "index = pinecone.Index(\"video-transcript-embeddings\")\n",
    "index.describe_index_stats()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment\n",
    "\n",
    "To experiment, change the query and run the following two cells - you can go to the reference video here:\n",
    "\n",
    "https://www.youtube.com/watch?v=jSP-gSEyVeI\n",
    "\n",
    "The last cell will return a link to the predicted timestamp, and say the timestamp too. Double check, because sometimes\n",
    "youtube caches your video progress and overrides the timestamp parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "query = \"how woudl I make a conversational agent in langchain\"\n",
    "similar_docs = db.similarity_search_with_score(query)\n",
    "\n",
    "similar_docs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(Document(page_content=\"really similar to the first one so let's take a look this is called conversational react and this is basically the last agent but with memory so we can interact with it in several interactions and ask it questions from things that we have already said it is really useful to have a chatbot basically it's the basic basis for for chatbots in Lang chain so it's it's really useful again we load the same tools here we will add memory as as we said so we will ask it a similar question a bit different\", metadata={'chunk_timestamp': 1463.1}),\n",
       "  0.33867186),\n",
       " (Document(page_content=\"and it can even perform SQL queries let's start with a very simple example of this what we're going to do is build a calculator agent that can also handle some general knowledge queries now to use agents in line chain we need three key components that is a large language model or multiple large language models a tool that we will be interacting with and an Asian to control the interaction let's start by installing Line train and initializing our large language model so we're in collab here\", metadata={'chunk_timestamp': 179.879}),\n",
       "  0.3477953),\n",
       " (Document(page_content=\"and there are things you can do you can create your own agent you can use agents with several other tools and another thing worth mentioning is that you can use a tracing UI tool that is within Lang chain which will allow you to understand within a beautiful UI how the agent is thinking on what different calls to different llms it did within its thought process so that is really convenient when you're using complex agents with several tools and it might be tricky to track what the whole thought\", metadata={'chunk_timestamp': 1857.72}),\n",
       "  0.36873192),\n",
       " (Document(page_content=\"solutions to these problems comes in the form of Agents these agents don't just solve many of the problems we saw above but actually many others as well in fact by using agents we actually have a almost unlimited upside in the potential of what we can do with large language models so we're going to learn what agents are and how we can use them within line chain library to superpower our large language models what we'll do is I'll quickly go through an introduction to agents in line chain and\", metadata={'chunk_timestamp': 111.299}),\n",
       "  0.39474416)]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "best_doc = similar_docs[-1][0]\n",
    "seconds = best_doc.metadata[\"chunk_timestamp\"]\n",
    "s = f\"https://www.youtube.com/watch?v=jSP-gSEyVeI&t={seconds}\"\n",
    "\n",
    "def convert_seconds(seconds):\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    return int(minutes), int(seconds)\n",
    "\n",
    "# Example usage:\n",
    "minutes, seconds = convert_seconds(seconds)\n",
    "\n",
    "\n",
    "print(s)\n",
    "print(f\"exact timestamp - {minutes}:{seconds}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.youtube.com/watch?v=jSP-gSEyVeI&t=111.299\n",
      "exact timestamp - 1:51\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "FAISS.from_embeddings() missing 2 required positional arguments: 'text_embeddings' and 'embedding'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: FAISS.from_embeddings() missing 2 required positional arguments: 'text_embeddings' and 'embedding'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.11.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11.3 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}